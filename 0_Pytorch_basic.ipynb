{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# *TENSOR MANIPULATION : 텐서 조작*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "1D TORCH\n",
      "********************\n",
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n",
      "\n",
      "\n",
      "********************\n",
      "2D TORCH\n",
      "********************\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n",
      "\n",
      "\n",
      "********************\n",
      "BROADCASTING\n",
      "********************\n",
      "tensor([[5., 5.]])\n",
      "tensor([[4., 5.]])\n",
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n",
      "\n",
      "\n",
      "********************\n",
      "CALCULATION\n",
      "********************\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor(1.5000)\n",
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1차원\n",
    "print('*'*20)\n",
    "print('1D TORCH')\n",
    "print('*'*20)\n",
    "\n",
    "t = torch.FloatTensor([0,1,2,3,4,5,6])\n",
    "print(t)\n",
    "print(t.dim())  # rank. 즉, 차원\n",
    "print(t.shape)  # shape\n",
    "print(t.size()) # shape\n",
    "print(t[0], t[1], t[-1])  # 인덱스로 접근\n",
    "print(t[2:5], t[4:-1])    # 슬라이싱\n",
    "print(t[:2], t[3:])       # 슬라이싱\n",
    "\n",
    "\n",
    "# 2차원\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('2D TORCH')\n",
    "print('*'*20)\n",
    "\n",
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "\n",
    "print(t)\n",
    "print(t.dim())  # rank. 즉, 차원\n",
    "print(t.size()) # shape\n",
    "print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.\n",
    "print(t[:, 1].size()) # ↑ 위의 경우의 크기\n",
    "print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다.\n",
    "\n",
    "\n",
    "# Broadcasting\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('BROADCASTING')\n",
    "print('*'*20)\n",
    "\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)\n",
    "\n",
    "# Vector + scalar\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3]) # [3] -> [3, 3]\n",
    "print(m1 + m2)\n",
    "\n",
    "# 2 x 1 Vector + 1 x 2 Vector\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([[3], [4]])\n",
    "print(m1 + m2)\n",
    "\n",
    "\n",
    "# Calculation\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('CALCULATION')\n",
    "print('*'*20)\n",
    "\n",
    "# matmul\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1.matmul(m2)) # 2 x 1\n",
    "\n",
    "# x.mul()\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1 * m2) # 2 x 2\n",
    "print(m1.mul(m2))\n",
    "\n",
    "# mean\n",
    "t = torch.FloatTensor([1, 2])\n",
    "print(t.mean())\n",
    "\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))\n",
    "\n",
    "# sum\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "print(t.sum()) # 단순히 원소 전체의 덧셈을 수행\n",
    "print(t.sum(dim=0)) # 행을 제거\n",
    "print(t.sum(dim=1)) # 열을 제거\n",
    "print(t.sum(dim=-1)) # 열을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "VIEW (=numpy.reshape)\n",
      "********************\n",
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "shape =  torch.Size([2, 2, 3])\n",
      "view 사용 ->  tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "view 사용 shape ->  torch.Size([4, 3])\n",
      "\n",
      "\n",
      "********************\n",
      "SQUEEZE : 1차원 제거\n",
      "********************\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "shape =  torch.Size([3, 1])\n",
      "Squeeze 사용 ->  tensor([0., 1., 2.])\n",
      "Squeeze 사용 shape ->  torch.Size([3])\n",
      "\n",
      "\n",
      "********************\n",
      "UNSQUEEZE : 1차원 추가\n",
      "********************\n",
      "tensor([0., 1., 2.])\n",
      "shape =  torch.Size([3])\n",
      "unsqueeze(0) 사용 ->  tensor([[0., 1., 2.]])\n",
      "unsqueeze(0) 사용 shape ->  torch.Size([1, 3])\n",
      "view 사용 ->  tensor([[0., 1., 2.]])\n",
      "view 사용 shape ->  torch.Size([1, 3])\n",
      "unsqueeze(1) 사용 ->  tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "unsqueeze(1) 사용 shape ->  torch.Size([3, 1])\n",
      "unsqueeze(-1) 사용 ->  tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "unsqueeze(-1) 사용 shape ->  torch.Size([3, 1])\n",
      "\n",
      "\n",
      "********************\n",
      "TYPE CASTING : 자료형 변환\n",
      "********************\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n",
      "\n",
      "\n",
      "********************\n",
      "CONCATENATE : 텐서 연결\n",
      "********************\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n",
      "\n",
      "\n",
      "********************\n",
      "STACKING : 텐서 쌓기\n",
      "********************\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "\n",
      "********************\n",
      "FILL : 텐서 채우기\n",
      "********************\n",
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "\n",
      "********************\n",
      "INPLACE OPERATION : 덮어쓰기\n",
      "********************\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "mul사용 저장X ->  tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "mul_사용 저장O ->  tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# View \n",
    "print('*'*20)\n",
    "print('VIEW (=numpy.reshape)')\n",
    "print('*'*20)\n",
    "\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft)\n",
    "print('shape = ',ft.shape)\n",
    "print('view 사용 -> ',ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경\n",
    "print('view 사용 shape -> ',ft.view([-1, 3]).shape)\n",
    "\n",
    "\n",
    "# Squeeze \n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('SQUEEZE : 1차원 제거')\n",
    "print('*'*20)\n",
    "\n",
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print('shape = ',ft.shape)\n",
    "print('Squeeze 사용 -> ',ft.squeeze())\n",
    "print('Squeeze 사용 shape -> ',ft.squeeze().shape)\n",
    "\n",
    "\n",
    "# Unsqueeze \n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('UNSQUEEZE : 1차원 추가')\n",
    "print('*'*20)\n",
    "\n",
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft)\n",
    "print('shape = ',ft.shape)\n",
    "print('unsqueeze(0) 사용 -> ',ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
    "print('unsqueeze(0) 사용 shape -> ',ft.unsqueeze(0).shape)\n",
    "\n",
    "print('view 사용 -> ',ft.view(1, -1)) # view로도 구현 가능\n",
    "print('view 사용 shape -> ',ft.view(1, -1).shape)\n",
    "\n",
    "print('unsqueeze(1) 사용 -> ',ft.unsqueeze(1))\n",
    "print('unsqueeze(1) 사용 shape -> ',ft.unsqueeze(1).shape)\n",
    "\n",
    "print('unsqueeze(-1) 사용 -> ',ft.unsqueeze(-1))\n",
    "print('unsqueeze(-1) 사용 shape -> ',ft.unsqueeze(-1).shape)\n",
    "\n",
    "\n",
    "# Type Casting \n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('TYPE CASTING : 자료형 변환')\n",
    "print('*'*20)\n",
    "\n",
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)\n",
    "print(lt.float())\n",
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt)\n",
    "print(bt.long())\n",
    "print(bt.float())\n",
    "\n",
    "\n",
    "# Concatenate : x.cat([])\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('CONCATENATE : 텐서 연결')\n",
    "print('*'*20)\n",
    "\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "print(torch.cat([x, y], dim=0))\n",
    "print(torch.cat([x, y], dim=1))\n",
    "\n",
    "\n",
    "# Stacking : x.stack([])\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('STACKING : 텐서 쌓기')\n",
    "print('*'*20)\n",
    "\n",
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "print(torch.stack([x, y, z]))\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0)) # stack과 동일한 작업\n",
    "print(torch.stack([x, y, z], dim=1))\n",
    "\n",
    "\n",
    "# Fill : ones_like / zeros_like\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('FILL : 텐서 채우기')\n",
    "print('*'*20)\n",
    "\n",
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)\n",
    "print(torch.ones_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 1로 채우기\n",
    "print(torch.zeros_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 0으로 채우기\n",
    "\n",
    "\n",
    "# In-place Operation : mul_\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('INPLACE OPERATION : 덮어쓰기')\n",
    "print('*'*20)\n",
    "\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력\n",
    "print('mul사용 저장X -> ',x) # 기존의 값 출력\n",
    "print(x.mul_(2.))  # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력\n",
    "print('mul_사용 저장O -> ',x) # 기존의 값 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# *CLASS*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "FUNCTION\n",
      "********************\n",
      "function1 ->  3 7\n",
      "function2 ->  3 10\n",
      "\n",
      "\n",
      "********************\n",
      "CLASS\n",
      "********************\n",
      "class1 ->  3 7\n",
      "class2 ->  3 10\n"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "print('*'*20)\n",
    "print('FUNCTION')\n",
    "print('*'*20)\n",
    "result1 = 0\n",
    "result2 = 0\n",
    "\n",
    "def add1(num):\n",
    "    global result1\n",
    "    result1 += num\n",
    "    return result1\n",
    "\n",
    "def add2(num):\n",
    "    global result2\n",
    "    result2 += num\n",
    "    return result2\n",
    "\n",
    "print('function1 -> ', add1(3), add1(4))\n",
    "print('function2 -> ', add2(3), add2(7))\n",
    "\n",
    "\n",
    "# Class\n",
    "print('\\n')\n",
    "print('*'*20)\n",
    "print('CLASS')\n",
    "print('*'*20)\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self): # 객체 생성 시 호출될 때 실행되는 초기화 함수. 이를 생성자라고 한다.\n",
    "        self.result = 0\n",
    "\n",
    "    def add(self, num): # 객체 생성 후 사용할 수 있는 함수.\n",
    "        self.result += num\n",
    "        return self.result\n",
    "    \n",
    "cal1 = Calculator()\n",
    "cal2 = Calculator()\n",
    "\n",
    "print('class1 -> ', cal1.add(3), cal1.add(4))\n",
    "print('class2 -> ', cal2.add(3), cal2.add(7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
